<!DOCTYPE html>
<html lang="en-ES">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Portfolio">
  <meta name="author" content="Maximiliano Benítez">
  <meta name="robots" content="noindex" />
  <title>Portfolio</title>
  <link href="css/styles.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Raleway&display=swap">
</head>

<body>
  <nav class="c-nav_web">
    <a href="index.html">
      <h1>Machine Learning Portfolio</h1>
    </a>
    <ul class="c-navigation__list">
      <li id="li-home" class="c-item" data-target-section="home">
        <a href="index.html">Inicio</a>
      </li>
      <li id="li-blog" class="c-item is-active" data-target-section="blog">
        <a href="blog.html">Blog</a>
      </li>
      <li id="li-case__studies" class="c-item" data-target-section="case__studies">
        <a href="case_studies.html">Casos de Estudio</a>
      </li>
    </ul>
  </nav>

  <nav class="c-nav_mobile">
    <a href="index.html">
      <h1>ML Portfolio</h1>
    </a>
    <ul class="c-navigation__list">
      <li id="li-home" class="c-item" data-target-section="home">
        <a href="index.html">I</a>
      </li>
      <li id="li-blog" class="c-item is-active" data-target-section="blog">
        <a href="blog.html">B</a>
      </li>
      <li id="li-case__studies" class="c-item" data-target-section="case__studies">
        <a href="case_studies.html">CE</a>
      </li>
    </ul>
  </nav>

  <main class="c-case__studies">
    <div class="c-header">
      <h2>Regresión Lineal</h2>
    </div>
    <div class="c-cases">
      <p>La regresión lineal es un algoritmo fundamental en estadísticas y aprendizaje automático. Aunque la regresión
        lineal se originó en estadísticas, también es ampliamente utilizada en el aprendizaje automático. Esto se debe a
        su utilidad en la predicción precisa y al equilibrio entre el error y la explicabilidad. La regresión lineal
        tiene varios nombres según el contexto. Se refiere a la relación lineal entre las variables de entrada y salida.
        Cuando hay una sola variable de entrada, se llama regresión lineal simple, y cuando hay múltiples variables, se
        llama regresión lineal múltiple.
      </p>
      <h4>Representación del Modelo de Regresión Lineal</h4>
      <p>La representación del modelo es una ecuación lineal que pondera las entradas (x) con coeficientes y un término
        de sesgo. Los coeficientes determinan la contribución relativa de cada entrada en la predicción. La regresión
        lineal puede extenderse a dimensiones superiores, como planos o hiperplanos.</p>
      <h4>Aprendiendo el Modelo de Regresión Lineal</h4>
      <p>Aprender un modelo de regresión lineal significa estimar los valores de los coeficientes utilizados en la
        representación con los datos que tenemos disponibles. Se pueden utilizar varios métodos para aprender el modelo
        de regresión lineal. Uno de los métodos comunes es el de Mínimos Cuadrados Ordinarios, que busca minimizar la
        suma de los residuos al cuadrado. El Descenso del Gradiente es otra técnica popular para optimizar los
        coeficientes.</p>
      <h5>Mínimos Cuadrados Ordinarios</h5>
      <ul class="c-bulleted">
        <li>Se utiliza cuando se tienen múltiples entradas en un modelo de regresión.</li>
        <li>Busca minimizar la suma de los residuos al cuadrado, que son las diferencias entre los valores observados y
          los valores predichos por el modelo.</li>
        <li>Trata los datos como una matriz y utiliza álgebra lineal para estimar los coeficientes óptimos del modelo.
        </li>
        <li>Es rápido de calcular cuando se tienen todos los datos disponibles.</li>
      </ul>
      <h5>Descenso del Gradiente</h5>
      <ul class="c-bulleted">
        <li>Utilizado para optimizar los coeficientes de un modelo iterativamente al minimizar el error en los datos de
          entrenamiento.</li>
        <li>Calcula los errores al cuadrado para cada par de entrada y salida y actualiza los coeficientes en la
          dirección que minimiza el error.</li>
        <li>Repite el proceso hasta alcanzar una suma mínima de errores cuadrados o hasta que no sea posible una mejora
          adicional.</li>
        <li>Requiere seleccionar un parámetro de tasa de aprendizaje (alfa) que determina el tamaño de los pasos de
          mejora en cada iteración.</li>
        <li>Útil cuando se trabaja con conjuntos de datos grandes que pueden no caber en la memoria.</li>
      </ul>
      <h4>Regresión Lineal Regularizada</h4>
      <p>La regresión lineal regularizada aborda el sobreajuste ajustando el modelo para minimizar la suma de errores
        cuadrados y la complejidad del modelo. Dos enfoques son Regresión Lasso y Regresión Ridge, que penalizan los
        coeficientes más grandes. Estos métodos son efectivos cuando hay colinealidad en tus valores de entrada y los
        mínimos cuadrados ordinarios sobreajustarían los datos de entrenamiento.</p>
      <h4>Realizar Predicciones con Regresión Lineal</h4>
      <p>Hacer predicciones con un modelo de regresión lineal implica sustituir los valores de entrada en la ecuación
        lineal del modelo. Cada coeficiente pondera la entrada correspondiente y el sesgo aporta una contribución
        constante.</p>
      <h4>Preparación de Datos para la Regresión Lineal</h4>
      <ul class="c-bulleted">
        <li>Suposición lineal</li>
        <li>Eliminar ruido (valores atípicos)</li>
        <li>Eliminar colinealidad</li>
        <li>Distribuciones gaussianas de variables</li>
        <li>Reescalar las entradas</li>
      </ul>
    </div>
  </main>

  <footer>
    <span>©2023 por Maximiliano Benítez. Creado en Github Pages</span>
  </footer>
</body>

</html>