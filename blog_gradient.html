<!DOCTYPE html>
<html lang="en-ES">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Portfolio">
  <meta name="author" content="Maximiliano Benítez">
  <meta name="robots" content="noindex" />
  <title>Portfolio</title>
  <link href="css/styles.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Raleway&display=swap">
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>
  <nav class="c-nav_web">
    <a href="index.html">
      <h1>Machine Learning Portfolio</h1>
    </a>
    <ul class="c-navigation__list">
      <li id="li-home" class="c-item" data-target-section="home">
        <a href="index.html">Inicio</a>
      </li>
      <li id="li-blog" class="c-item is-active" data-target-section="blog">
        <a href="blog.html">Blog</a>
      </li>
      <li id="li-case__studies" class="c-item" data-target-section="case__studies">
        <a href="case_studies.html">Casos de Estudio</a>
      </li>
    </ul>
  </nav>

  <nav class="c-nav_mobile">
    <a href="index.html">
      <h1>ML Portfolio</h1>
    </a>
    <ul class="c-navigation__list">
      <li id="li-home" class="c-item" data-target-section="home">
        <a href="index.html">I</a>
      </li>
      <li id="li-blog" class="c-item is-active" data-target-section="blog">
        <a href="blog.html">B</a>
      </li>
      <li id="li-case__studies" class="c-item" data-target-section="case__studies">
        <a href="case_studies.html">CE</a>
      </li>
    </ul>
  </nav>

  <main class="c-case__studies">
    <div class="c-header">
      <h2>Descenso de Gradiente</h2>
    </div>
    <div class="c-cases">
      <p>El descenso de gradiente es un algoritmo fundamental en el aprendizaje automático y la optimización de
        funciones. Se utiliza para encontrar los valores de los parámetros que minimizan una función de costo.</p>
      <h4>Descenso de Gradiente</h4>
      <p>Es un algoritmo de optimización para encontrar los parámetros que minimizan una función de costo. Es
        especialmente útil cuando no se pueden calcular los parámetros de manera analítica y se requiere un enfoque de
        optimización.</p>
      <h5>Intuición para el Descenso de Gradiente</h5>
      <p>El objetivo es ir probando diferentes valores para los coeficientes, evaluar su costo y seleccionar nuevos
        coeficientes que tengan un costo ligeramente mejor (más bajo). Repetir este proceso varias veces conducirá a
        los coeficientes que resultan en el costo mínimo.</p>
      <h5>Procedimiento de Descenso de Gradiente</h5>
      <p>Se inicia con valores iniciales para los coeficientes.</p>
      <p>$$coeficiente = 0.0$$</p>
      <p>El costo de los coeficientes se evalúa al insertarlos en la función y calcular el costo.</p>
      <p>$$costo = f(coeficiente)$$</p>
      <p>Se calcula la derivada del costo. La derivada se refiere a la pendiente y necesitamos conocer la pendiente
        para saber la dirección (signo) en la que mover los valores de los coeficientes para obtener un costo más bajo
        en la siguiente iteración.</p>
      <p>$$delta = derivada(costo)$$</p>
      <p>Ahora podemos actualizar los valores de los coeficientes. Se debe especificar un parámetro de tasa de
        aprendizaje (alfa) que controle cuánto pueden cambiar los coeficientes en cada actualización.</p>
      <p>$$coeficiente = coeficiente - (alfa \times delta)$$</p>
      <p>Este proceso se repite hasta que el costo de los coeficientes (costo) sea 0.0 o no se puedan lograr más
        mejoras en el costo.</p>
      <h4>Descenso de Gradiente por Lotes</h4>
      <p>En algoritmos supervisados, se busca ajustar una función objetivo. La función de costo se calcula para todo
        el conjunto de entrenamiento, y los coeficientes se actualizan al final de cada lote de datos (lote se le
        llama a una iteración del algoritmo).
        Ejemplos comunes de algoritmos con coeficientes que se pueden optimizar utilizando el descenso de gradiente
        son la Regresión Lineal y la Regresión Logística.</p>
      <h4>Descenso de Gradiente Estocástico</h4>
      <p>En conjuntos de datos grandes, el descenso de gradiente puede ser lento. El descenso de gradiente estocástico
        actualiza los coeficientes para cada instancia de entrenamiento, siendo más rápido pero ruidoso.</p>
      <p>$$prediction = \frac {1}{1+e^-(B0+B1\times X1+B2\times X2)}$$</p>
      <h4>Consejos para el Descenso del Gradiente</h4>
      <ul class="c-bulleted">
        <li>Graficar costo frente al tiempo</li>
        <li>Ajustar la tasa de aprendizaje</li>
        <li>Reescalar las entradas</li>
        <li>Considerar pocas pasadas</li>
        <li>Graficar el costo promedio</li>
      </ul>
    </div>
  </main>

  <footer>
    <span>©2023 por Maximiliano Benítez. Creado en Github Pages</span>
  </footer>
</body>

</html>